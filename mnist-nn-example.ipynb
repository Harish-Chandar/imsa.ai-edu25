{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-09T01:11:37.728175Z",
     "start_time": "2026-02-09T01:11:37.724109Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ],
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T01:14:24.990840Z",
     "start_time": "2026-02-09T01:14:24.987455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importing the data\n",
    "from torchvision import datasets, transforms"
   ],
   "id": "7842790a6e163845",
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T01:14:25.501990Z",
     "start_time": "2026-02-09T01:14:25.421575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Separately loading the training and test data using the MNIST dataset from torchvision. The data will be downloaded to the 'data' folder if it is not already there. We also apply a transformation to convert the images to tensors and normalize the pixel values to be between 0 and 1 (instead of 0-255).\n",
    "train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "x_train = train_data.data.float() / 255.0\n",
    "y_train = train_data.targets.long()\n",
    "x_test = test_data.data.float() / 255.0\n",
    "y_test = test_data.targets.long()"
   ],
   "id": "3477f2848f8315db",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T01:14:37.883807Z",
     "start_time": "2026-02-09T01:14:37.873565Z"
    }
   },
   "cell_type": "code",
   "source": "x_train[0] #Single Image (28x28 pixel values, each value between 0-255)",
   "id": "3511518b8777f344",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706, 0.4941, 0.5333,\n",
       "         0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1176,\n",
       "         0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "         0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922, 0.9333,\n",
       "         0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9843,\n",
       "         0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.8588,\n",
       "         0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137, 0.9686, 0.9451,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3137,\n",
       "         0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000, 0.1686, 0.6039,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275, 0.4235, 0.0039,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922, 0.9922, 0.4667,\n",
       "         0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294, 0.9922, 0.9922,\n",
       "         0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627, 0.3647, 0.9882,\n",
       "         0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9765,\n",
       "         0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098, 0.7176, 0.9922,\n",
       "         0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922, 0.9922, 0.9922,\n",
       "         0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922, 0.9922, 0.7882,\n",
       "         0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0902,\n",
       "         0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.3176, 0.0078,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706, 0.8588,\n",
       "         0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922, 0.9922,\n",
       "         0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922, 0.8314,\n",
       "         0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T01:14:38.451793Z",
     "start_time": "2026-02-09T01:14:38.448109Z"
    }
   },
   "cell_type": "code",
   "source": "y_train #Output data: Label of the image (0-9)",
   "id": "e92e9e8f9b570261",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T01:14:39.017703Z",
     "start_time": "2026-02-09T01:14:38.942963Z"
    }
   },
   "cell_type": "code",
   "source": "plt.imshow(x_train[0]) #Plotting the first image in the training data",
   "id": "6c4441e096d207df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b3516a82d0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T01:14:39.440231Z",
     "start_time": "2026-02-09T01:14:39.432990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Building the model\n",
    "# Using a sequential model because the layers are connected one after the other.\n",
    "model = torch.nn.Sequential()\n",
    "\n",
    "class MNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "\n",
    "        # Adding a flatten layer to convert the 2D data from the input images into a 1D array (so we can use it in a dense layer because dense layers only take 1D input). The input shape is (28, 28) because the images are 28x28 pixels.\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Adding a dense layer to combine the features extracted by the previous layers and learn complex patterns. This layer has 128 neurons and uses ReLU activation.\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        # Adding a dropout layer to randomly set 20% of the inputs to 0 during training, which helps to prevent overfitting by making the model less reliant on specific neurons.\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        # Same as above but with 64 neurons instead of 128 to reduce the complexity of the model and make it less likely to overfit.\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        # Adding a dense output layer with 10 neurons (one for each number) and softmax activation to get probabilities for which number the image is of. We use the dense layer here because we want to combine all the features learned by the previous layers to make a final prediction. The softmax activation is used because we want the output to be a probability distribution over the 10 classes (Numbers 0-9).\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    # The forward method defines how the input data flows through the layers of the model. We apply the flatten layer to convert the input images into a 1D array, then pass it through the first dense layer and apply ReLU activation, followed by dropout. We repeat this process for the second dense layer, and finally pass it through the output layer and apply softmax activation to get the final probabilities for each class.\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ],
   "id": "cfafacf7560337ae",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T01:16:55.645709Z",
     "start_time": "2026-02-09T01:16:33.900231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instantiate model\n",
    "torch.manual_seed(42)\n",
    "model = MNISTModel()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Creating data loaders for training and testing data. Data loaders are used to load the data in batches during training, which helps to speed up the training process and allows us to use larger datasets that may not fit into memory all at once. We set shuffle=True for the training data to ensure that the model sees the data in a different order each epoch, which can help to improve generalization.\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader: # Loop through the training data in batches\n",
    "        optimizer.zero_grad() # Clear the gradients from the previous step to prevent accumulation\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward() # Compute the gradients of the loss with respect to the model's parameters\n",
    "        optimizer.step() # Update the model's parameters using the computed gradients\n",
    "\n",
    "        running_loss += loss.item() # Accumulate the loss for this epoch to compute the average loss later\n",
    "        _, predicted = torch.max(outputs, 1) # Get the predicted class by finding the index of the maximum output value for each sample in the batch\n",
    "        total += labels.size(0) # Update the total number of samples seen so far (the batch size)\n",
    "        correct += (predicted == labels).sum().item() # Update the count of correct predictions by comparing the predicted classes to the true labels and summing the number of matches\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "4030698a1159c16a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.6038, Accuracy: 87.13%\n",
      "Epoch 2/5, Loss: 1.5298, Accuracy: 93.52%\n",
      "Epoch 3/5, Loss: 1.5165, Accuracy: 94.66%\n",
      "Epoch 4/5, Loss: 1.5087, Accuracy: 95.36%\n",
      "Epoch 5/5, Loss: 1.5045, Accuracy: 95.78%\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T02:12:46.222604Z",
     "start_time": "2026-02-09T02:12:46.193068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# After training the model, we can evaluate its performance on the test set to see how well it generalizes to unseen data. We disable gradients during evaluation since we are not updating the model's parameters, which can speed up the evaluation process and reduce memory usage.\n",
    "model.eval()\n",
    "with torch.no_grad():  # disable gradients for evaluation\n",
    "    predicted = model(x_test)          # pass in test images\n",
    "    test_loss = criterion(predicted, y_test)  # compute loss\n",
    "    _, predicted_classes = torch.max(predicted, 1)  # get class predictions\n",
    "    test_accuracy = (predicted_classes == y_test).float().mean() * 100  # accuracy in percent\n",
    "\n",
    "print(f'Test Loss: {test_loss.item():.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n"
   ],
   "id": "422543ebafbaa569",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.4958\n",
      "Test Accuracy: 96.61%\n"
     ]
    }
   ],
   "execution_count": 122
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
